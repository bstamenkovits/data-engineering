{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e34e60d-2724-4530-a010-d33554458b4a",
   "metadata": {},
   "source": [
    "# **Manipulating Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f41314-2102-4794-ae42-cfcfc175ade3",
   "metadata": {},
   "source": [
    "When manipulating data in Spark one could transform a dataframe into a Temporary View, and then use `spark.sql()` to perform various SQL operations. However the spark DataFrame object has a lot of the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c42e1fa-4e2f-4264-a91a-697a2e4a0396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a163d03c-092f-48e5-9672-fc7d3f69f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = spark = SparkSession.builder.appName('example app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26bcf05d-1737-4409-a213-bdc4c0bd2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+------------+-------------+\n",
      "|          name|continent|code|surface_area|geosize_group|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "|   Afghanistan|     Asia| AFG|      652090|       medium|\n",
      "|   Netherlands|   Europe| NLD|       41526|        small|\n",
      "|       Albania|   Europe| ALB|       28748|        small|\n",
      "|       Algeria|   Africa| DZA|     2381740|        large|\n",
      "|American Samoa|  Oceania| ASM|         199|        small|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"data/countries.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f72791-c6f3-41c5-ab36-90f72ebaf6ba",
   "metadata": {},
   "source": [
    "## Select Data\n",
    "In order to select data, the `.select()` method may be used. This method takes either\n",
    "* strings containing the column names to be selected\n",
    "* spark DataFrame column objects representing the columns to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f0c04d3-7eb6-47f1-a370-137b2c320a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|          name|continent|\n",
      "+--------------+---------+\n",
      "|   Afghanistan|     Asia|\n",
      "|   Netherlands|   Europe|\n",
      "|       Albania|   Europe|\n",
      "|       Algeria|   Africa|\n",
      "|American Samoa|  Oceania|\n",
      "+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these two operations are equivalent\n",
    "data = df.select(df.name, df.continent)\n",
    "data = df.select('name', 'continent')\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d75aca-0541-466e-a9ce-3747976d1dd8",
   "metadata": {},
   "source": [
    "## Filter Data\n",
    "In order to filter data, the `.filter()` method may be used. This method takes either \n",
    "* a string containing an expression that normally follows the `WHERE` clause in SQL\n",
    "* a spark column of boolean values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0814f6e0-6c05-4242-a05c-5c9543b95e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+------------+-------------+\n",
      "|                name|continent|code|surface_area|geosize_group|\n",
      "+--------------------+---------+----+------------+-------------+\n",
      "|         Afghanistan|     Asia| AFG|      652090|       medium|\n",
      "|United Arab Emirates|     Asia| ARE|       83600|        small|\n",
      "|             Armenia|     Asia| ARM|       29800|        small|\n",
      "|          Azerbaijan|     Asia| AZE|       86600|        small|\n",
      "|             Bahrain|     Asia| BHR|         694|        small|\n",
      "+--------------------+---------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# these two operations are equivalent\n",
    "filtered_data = df.filter('continent = \"Asia\"')\n",
    "filtered_data = df.filter(df.continent == 'Asia')\n",
    "\n",
    "filtered_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3d39f-ed90-4efa-86c5-7809a16d199c",
   "metadata": {},
   "source": [
    "## Adding or Updating Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bffa20-03ac-4751-8645-81df03084b39",
   "metadata": {},
   "source": [
    "### Using `.withColumn()`\n",
    "In order to add columns, the `.withColumn()` method may be used. The first argument is the name of the new column, the second argument is a Spark DataFrame column object. \n",
    "\n",
    "If the name of the new column is already present in the DataFrame, then that column will be updated with the new values. \n",
    "\n",
    "It should be noted that the returning dataframe will always be the entire original dataframe with only change being the new or updated column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1897e752-18ed-443a-8fd1-2aa800b86e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+------------+-------------+\n",
      "|          name|continent|code|surface_area|geosize_group|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "|   Afghanistan|     Asia| AFG|    652091.0|       medium|\n",
      "|   Netherlands|   Europe| NLD|     41527.0|        small|\n",
      "|       Albania|   Europe| ALB|     28749.0|        small|\n",
      "|       Algeria|   Africa| DZA|   2381741.0|        large|\n",
      "|American Samoa|  Oceania| ASM|       200.0|        small|\n",
      "+--------------+---------+----+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+---------+----+------------+-------------+---------------------+\n",
      "|          name|continent|code|surface_area|geosize_group|adjusted_surface_area|\n",
      "+--------------+---------+----+------------+-------------+---------------------+\n",
      "|   Afghanistan|     Asia| AFG|      652090|       medium|             652091.0|\n",
      "|   Netherlands|   Europe| NLD|       41526|        small|              41527.0|\n",
      "|       Albania|   Europe| ALB|       28748|        small|              28749.0|\n",
      "|       Algeria|   Africa| DZA|     2381740|        large|            2381741.0|\n",
      "|American Samoa|  Oceania| ASM|         199|        small|                200.0|\n",
      "+--------------+---------+----+------------+-------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update existing column\n",
    "df.withColumn('surface_area', df.surface_area+1).show(5)\n",
    "\n",
    "# add new column\n",
    "df.withColumn('adjusted_surface_area', df.surface_area+1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b17fdac-5a25-493e-bfc1-b695e3d2cc9f",
   "metadata": {},
   "source": [
    "### Using `.select()`\n",
    "Alternatively the `.select()` method may be used in combination with `.alias()`. In this case the `select` method will take a new spark DataFrame column as argument, and the `alias` method will take the new column name as argument. \n",
    "\n",
    "It should be noted that with this method only the column names passed to the `select` method will be returned. This can be advantageous as this will ensure no unnecessary data is kept in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976b7010-3df4-402e-b576-67113120ddad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------------------+\n",
      "|          name|continent|adjusted_surface_area|\n",
      "+--------------+---------+---------------------+\n",
      "|   Afghanistan|     Asia|             652091.0|\n",
      "|   Netherlands|   Europe|              41527.0|\n",
      "|       Albania|   Europe|              28749.0|\n",
      "|       Algeria|   Africa|            2381741.0|\n",
      "|American Samoa|  Oceania|                200.0|\n",
      "+--------------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    'name',\n",
    "    'continent',\n",
    "    (df.surface_area+1).alias('adjusted_surface_area')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea5e5b-84c6-466c-b71c-d6abb8bd85f5",
   "metadata": {},
   "source": [
    "### Using `.selectExpr()`\n",
    "Finally the same result can be achieved with the `.selectExpr()` method, which simply takes a SQL `SELECT` statement as argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f09cfda5-0b08-42b6-8a23-fdbdc9380221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------------------+\n",
      "|          name|continent|adjusted_surface_area|\n",
      "+--------------+---------+---------------------+\n",
      "|   Afghanistan|     Asia|             652091.0|\n",
      "|   Netherlands|   Europe|              41527.0|\n",
      "|       Albania|   Europe|              28749.0|\n",
      "|       Algeria|   Africa|            2381741.0|\n",
      "|American Samoa|  Oceania|                200.0|\n",
      "+--------------+---------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "        'name',\n",
    "        'continent',\n",
    "        \"surface_area + 1 AS adjusted_surface_area\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf48d1c-91fd-4df6-915a-f60e94394da0",
   "metadata": {},
   "source": [
    "## Aggregating Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99a4bd-801b-4ed5-a2b3-ac025bfa724e",
   "metadata": {},
   "source": [
    "Similar to SQL and Pandas DataFrames, aggregate values can be determined by first grouping the data. The `.groupBy()` method creates a `GroupedData` object, which has common aggregation methods like (but not limited to)\n",
    "* `sum()`\n",
    "* `min()`\n",
    "* `max()`\n",
    "* `count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df2e899-0959-4790-9dfb-8a08241926df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|    continent|average_surface_area|\n",
      "+-------------+--------------------+\n",
      "|       Europe|   539193.9880952381|\n",
      "|       Africa|   531466.1923076923|\n",
      "|North America|   834824.9655172414|\n",
      "|South America|  1480229.0833333333|\n",
      "|      Oceania|   475701.8888888889|\n",
      "|         Asia|   649590.7346938775|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# prepare data\n",
    "continents = [\n",
    "    'Europe', \n",
    "    'Asia', \n",
    "    'Africa', \n",
    "    'Oceania', \n",
    "    'North America', \n",
    "    'South America'\n",
    "]\n",
    "data = (\n",
    "    df\n",
    "    .withColumn('surface_area', df.surface_area.cast('double'))\n",
    "    .filter(df.continent.isin(continents))\n",
    ")\n",
    "\n",
    "# these two expressions are equivalent\n",
    "result = (\n",
    "    data\n",
    "    .groupBy('continent')\n",
    "    .avg('surface_area')\n",
    "    .withColumnRenamed('avg(surface_area)', 'average_surface_area')\n",
    ")\n",
    "result = (\n",
    "    data\n",
    "    .groupBy('continent')\n",
    "    .agg(\n",
    "        avg('surface_area').alias('average_surface_area')  # use imported avg() function\n",
    "    )\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44ef0df-2f49-45be-90c8-e587f3b7721b",
   "metadata": {},
   "source": [
    "## Joining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226f178-23b5-4a31-84b6-c1ee3b39511b",
   "metadata": {},
   "source": [
    "Two tables can be joined together using the `.join` method. This takes three arguments:\n",
    "* `other`: the other DataFrame\n",
    "* `on`: a string indicating which column to join on\n",
    "* `how`: the type of join (inner, outer, full, cross, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88b4e7d-a38e-4a25-961c-e5f340e355da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----+------------+-------------+\n",
      "|       name|continent|code|surface_area|geosize_group|\n",
      "+-----------+---------+----+------------+-------------+\n",
      "|Afghanistan|     Asia| AFG|      652090|       medium|\n",
      "|Netherlands|   Europe| NLD|       41526|        small|\n",
      "|    Albania|   Europe| ALB|       28748|        small|\n",
      "+-----------+---------+----+------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+------------+---------------+-------------+-------------+\n",
      "|     name|country_code|city_proper_pop|metroarea_pop|urbanarea_pop|\n",
      "+---------+------------+---------------+-------------+-------------+\n",
      "|  Abidjan|         CIV|        4765000|         null|      4765000|\n",
      "|Abu Dhabi|         ARE|        1145000|         null|      1145000|\n",
      "|    Abuja|         NGA|        1235880|      6000000|      1235880|\n",
      "+---------+------------+---------------+-------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = spark.read.csv(\"data/countries.csv\", header=True, inferSchema=True)\n",
    "cities = spark.read.csv(\"data/cities.csv\", header=True, inferSchema=True)\n",
    "\n",
    "countries.show(3)\n",
    "cities.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a453a5-d6a6-4bbe-97bd-7a2c02f82a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------+\n",
      "|  city_name|        country_name|continent|\n",
      "+-----------+--------------------+---------+\n",
      "|    Abidjan|       Cote d'Ivoire|   Africa|\n",
      "|  Abu Dhabi|United Arab Emirates|     Asia|\n",
      "|      Abuja|             Nigeria|   Africa|\n",
      "|      Accra|               Ghana|   Africa|\n",
      "|Addis Ababa|            Ethiopia|   Africa|\n",
      "+-----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries = countries.withColumnRenamed('name', 'country_name')\n",
    "cities = cities.withColumnRenamed('name', 'city_name')\n",
    "\n",
    "result = cities.join(\n",
    "    countries, \n",
    "    on=(countries.code == cities.country_code),\n",
    "    how='inner',\n",
    ").select('city_name', 'country_name', 'continent')\n",
    "\n",
    "result.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
